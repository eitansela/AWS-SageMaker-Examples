{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049; text-align: center; border-radius: 5px 5px; padding: 5px\"> Hugging Face Transformers with Amazon SageMaker and Multi-Model Endpoints </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"img/multi-model.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Deploy multiple Transformer models to the same Amazon SageMaker Infrastructure </h2>\n",
    "\n",
    "We can hosts up to thousands of models with [Amazon SageMaker multi-model endpoints](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html).\n",
    "\n",
    "**Use cases of multi-model endpoints:**\n",
    "\n",
    "- Multiple models which can be served from a common inference container, can be invoked on-demand and we experience some additional latency for infrequently invoked models.\n",
    "- In some cases where the variable latency is tolerable, and cost optimization is more important, we may also decide to use MMEs for A/B/n testing, in place of the more typical production variant based strategy discussed here.\n",
    "\n",
    "**How multi-model endpoints work?**\n",
    "\n",
    "Amazon SageMaker takes care of the loading and unloading of models for a multi-model endpoint in the container’s memory, as they are invoked. When SageMaker receives an inference request for a particular model, it does the following.\n",
    "\n",
    "1. Routes the request to an instance assigned to that model.\n",
    "2. Instead of downloading all of the models from an Amazon S3 bucket to the container, SageMaker downloads only the invoked model from S3 bucket to that instance’s storage volume.\n",
    "3. Also loads the model to the container’s memory on that instance. If the model is already loaded in the container’s memory then invocation is performed immediately because SageMaker already downloaded that model and loaded it.\n",
    "\n",
    "Let’s say SageMaker has already loaded 100 models and needs to load another model into memory but instance’s memory utilization is high then what next?\n",
    "\n",
    "1. SageMaker unloads unused models from that instance’s container memory to ensure that there is enough memory to load the model.\n",
    "2. Models which are unloaded remain on the instance’s storage volume and can be loaded into the container’s memory later without being downloaded again from the S3 bucket.\n",
    "\n",
    "What if the instance’s storage volume reaches its capacity?\n",
    "\n",
    "Then SageMaker deletes the all unused models from the instance storage storage volume. We see SageMaker is much smarter in loading and unloading models to the container’s memory.\n",
    "\n",
    "If we want to add a new model to multi-model endpoint then we upload it to S3 and invoke it. If we want to delete a existing model from multi-model endpoint then stop sending requests and delete it from S3.\n",
    "\n",
    "We will use the Hugging Face Inference DLCs and Amazon SageMaker to deploy multiple transformer models as Multi-Model Endpoint. Amazon SageMaker Multi-Model Endpoint can be used to improve endpoint utilization and optimize costs.\n",
    "\n",
    "This notebook demonstrates how to host 2 pretrained transformers model in one container behind one endpoint.\n",
    "1. Use BERT model to extract embeddings from text.\n",
    "2. Use GPT-2 model to generate synthetic text for a given text.\n",
    "\n",
    "**NOTE**: As the time of writing this only `CPU` Instances are supported for Multi-Model Endpoint.\n",
    "\n",
    "Please refer to [Medium article](https://medium.com/@vinayakshanawad/multi-model-endpoints-with-hugging-face-transformers-and-amazon-sagemaker-c0e5a3693fac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Development Environment and Permissions </h2>\n",
    "\n",
    "NOTE: You can run this demo in Sagemaker Studio, your local machine, or Sagemaker Notebook Instances\n",
    "\n",
    "If you are going to use Sagemaker in a local environment (not SageMaker Studio or Notebook Instances). You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'huggingface-multimodel-deploy'\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Retrieve Model Artifacts </h2>\n",
    "\n",
    "#### `BERT model`\n",
    "\n",
    "First we will download the model artifacts for the pretrained [BERT](https://arxiv.org/abs/1810.04805) model. BERT is a popular natural language processing (NLP) model that extracts meaning and context from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.17.0 ipywidgets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model_path = 'models/bertmodel/model'\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)\n",
    "    \n",
    "model.save_pretrained(save_directory=model_path)\n",
    "tokenizer.save_pretrained(save_directory=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `GPT-2 model`\n",
    "\n",
    "Second we will download the model artifacts for the pretrained [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) model. GPT-2 is a popular text generation model that was developed by OpenAI. Given a text prompt it can generate synthetic text that may follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "model_path = 'models/gptmodel/model'\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)\n",
    "    \n",
    "model.save_pretrained(save_directory=model_path)\n",
    "tokenizer.save_vocabulary(save_directory=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Write the Inference Script </h2> \n",
    "\n",
    "#### `BERT model`\n",
    "\n",
    "Since we are bringing a model to SageMaker, we must create an inference script. The script will run inside our HuggingFace container. Our script should include a function for model loading, and optionally functions generating predicitions, and input/output processing. The HuggingFace container provides default implementations for generating a prediction and input/output processing. By including these functions in your script you are overriding the default functions. You can find additional [details here](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#serve-a-pytorch-model).\n",
    "\n",
    "**NOTE**:\n",
    "\n",
    "1. Single model deployment: To install additional libraries at container startup, we can add a requirements.txt text file that specifies the libraries to be installed using pip. Within the archive, the HuggingFace container expects all inference code and requirements.txt file to be inside the code/ directory.\n",
    "\n",
    "2. Multi-model deployment: To install additional libraries on the container, libraries which are part of requirements.txt text file needs to be installed using pip in inference script. Within the archive, the HuggingFace container expects all inference code to be inside the code/ directory.\n",
    "\n",
    "In the next cell we'll see our inference script for BERT model which helps us to extract embeddings from text. \n",
    "\n",
    "You will notice that it uses the [transformers library from Hugging Face](https://huggingface.co/docs/transformers/index) and installed using pip command in inference script, likewise we need to install additional libraries if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models/bertmodel/code\n",
    "\n",
    "! cp source_dir/model1/inference.py models/bertmodel/code/inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize models/bertmodel/code/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `GPT-2 model`\n",
    "\n",
    "In the next cell we'll see our inference script for GPT-2 model which helps us to generate synthetic text for a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models/gptmodel/code\n",
    "\n",
    "! cp source_dir/model2/inference.py models/gptmodel/code/inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize models/gptmodel/code/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Package Models </h2> \n",
    "\n",
    "For hosting, SageMaker requires that the deployment package be structed in a compatible format. It expects all files to be packaged in a tar archive named \"model.tar.gz\" (We name it as bertmodel.tar.gz an gptmodel.tar.gz) with gzip compression. Within the archive, the HuggingFace container expects all inference code file to be inside the code/ directory. See the guide here for a thorough explanation of the required directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -czvf models/bertmodel.tar.gz -C models/bertmodel/ .\n",
    "!tar -czvf models/gptmodel.tar.gz -C models/gptmodel/ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Upload multiple HuggingFace models to S3 </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "models_path = 's3://{0}/{1}/models'.format(bucket,prefix)\n",
    "\n",
    "S3Uploader.upload('models/bertmodel.tar.gz', models_path)\n",
    "S3Uploader.upload('models/gptmodel.tar.gz', models_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Create Multi-Model Endpoint </h2> \n",
    "\n",
    "After we upload BERT model to S3 we can deploy our endpoint. To create/deploy a real-time endpoint with boto3 you need to create a \"SageMaker Model\", a \"SageMaker Endpoint Configuration\" and a \"SageMaker Endpoint\". The \"SageMaker Model\" contains our multi-model configuration including our S3 path where we upload/deploy multiple huggingface models. The \"SageMaker Endpoint Configuration\" contains the configuration for the endpoint. The \"SageMaker Endpoint\" is the actual endpoint.\n",
    "\n",
    "Verify `multi-models` LABEL in docker file to indicate any pre-built container is capable of loading and serving multiple models concurrently.\n",
    "\n",
    "LABEL com.amazonaws.sagemaker.capabilities.multi-models=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodels_path = 's3://{0}/{1}/models/'.format(bucket,prefix)\n",
    "multimodels_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SageMaker Model\n",
    "image_uri = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:1.10.2-transformers4.17.0-cpu-py38-ubuntu20.04\"\n",
    "deployment_name = \"huggingface-multi-model\"\n",
    "\n",
    "primary_container = {\n",
    "    'Image': image_uri,\n",
    "    'Mode': 'MultiModel',\n",
    "    'ModelDataUrl': multimodels_path,\n",
    "    'Environment': {\n",
    "        'SAGEMAKER_PROGRAM': 'inference.py',\n",
    "        'SAGEMAKER_REGION': region,\n",
    "        'SAGEMAKER_SUBMIT_DIRECTORY': multimodels_path\n",
    "    }\n",
    "}\n",
    "\n",
    "create_model_response = sm_client.create_model(ModelName = deployment_name,\n",
    "                                              ExecutionRoleArn = get_execution_role(),\n",
    "                                              PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])\n",
    "\n",
    "# create SageMaker Endpoint configuration\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = f\"{deployment_name}-epc\",\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "        'InstanceType':'ml.m5.4xlarge',\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName': deployment_name,\n",
    "        'VariantName':'AllTraffic',\n",
    "        'InitialVariantWeight':1\n",
    "        }\n",
    "    ])\n",
    "\n",
    "print('Endpoint configuration arn:  {}'.format(endpoint_config_response['EndpointConfigArn']))\n",
    "\n",
    "# create SageMaker Endpoint\n",
    "endpoint_params = {\n",
    "    'EndpointName': f\"{deployment_name}-ep\",\n",
    "    'EndpointConfigName': f\"{deployment_name}-epc\",\n",
    "}\n",
    "endpoint_response = sm_client.create_endpoint(**endpoint_params)\n",
    "print('EndpointArn = {}'.format(endpoint_response['EndpointArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Get Predictions </h2> \n",
    "\n",
    "#### `BERT model`\n",
    "Now that our API endpoint is deployed, we can send it text to get predictions from our BERT model. You can use the SageMaker SDK or the SageMaker Runtime API to invoke the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "invoke_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "prompt = \"The best part of Amazon SageMaker is that it makes machine learning easy.\"\n",
    "\n",
    "response = invoke_client.invoke_endpoint(EndpointName=f\"{deployment_name}-ep\",\n",
    "                              TargetModel='bertmodel.tar.gz',\n",
    "                              Body=prompt.encode(encoding='UTF-8'),\n",
    "                              ContentType='text/csv')\n",
    "\n",
    "response['Body'].read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `GPT-2 model`\n",
    "\n",
    "Now that our RESTful API endpoint is deployed, we can send it text to get predictions from our GPT-2 model. You can use the SageMaker Python SDK or the SageMaker Runtime API to invoke the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "invoke_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "prompt = \"Working with SageMaker makes machine learning \"\n",
    "\n",
    "response = invoke_client.invoke_endpoint(EndpointName=f\"{deployment_name}-ep\",\n",
    "                              TargetModel='gptmodel.tar.gz', \n",
    "                              Body=json.dumps(prompt),\n",
    "                              ContentType='text/csv')\n",
    "\n",
    "response['Body'].read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Dynamically deploying models and Updating a model to the endpoint </h2> \n",
    "\n",
    "To dynamically deploy a model and update a model, you would follow the same approach as above and add it as a new model. For example, if you have retrained the bertmodel.tar.gz model and wanted to start invoking it, you would upload the updated model artifacts behind the following S3 prefix with a new name such as bertmodel_v2.tar.gz, and then change the TargetModel field to invoke bertmodel_v2.tar.gz instead of bertmodel.tar.gz.\n",
    "\n",
    "multimodels_path = 's3://sagemaker-us-east-2-(account-id)/huggingface-multimodel-deploy/models/'\n",
    "\n",
    "You should avoid overwriting model artifacts in Amazon S3, because the old version of the model might still be loaded in the endpoint's running container(s) or on the storage volume of instances on the endpoint: This would lead invocations to still use the old version of the model.\n",
    "\n",
    "Alternatively, you could stop the endpoint and re-deploy a fresh set of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Delete the Multi-Model Endpoint </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_model(ModelName=deployment_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=f\"{deployment_name}-epc\")\n",
    "sm_client.delete_endpoint(EndpointName=f\"{deployment_name}-ep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Conclusion </h2> \n",
    "\n",
    "We successfully deployed two Hugging Face Transformers to Amazon SageMaker for inference using the Multi-Model Endpoint. Multi-Model Endpoints are a great option to optimize compute utilization and costs for your models. Especially when you have independent inference workloads due to use-case differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for reading! If you have any questions, feel free to contact me."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.10 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.10-cpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
